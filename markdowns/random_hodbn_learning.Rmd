---
title: "Random HO-DBN learning"
author: "David Quesada"
date: "10/2/2021"
output: html_document
---

# Recovering HO-DBN structures from sampled data

In this rmarkdown, we will make some experiments to test the capabilities of the _psoho_, _natPsoho_ and dynamic MMHC algorithms in learning HO-DBNs with increasing Markovian order and number of receiving variables. Specifically, we are going to:

* Randomly generate a HO-DBN structure with order $m$ and $k$ receiving variables, crop the arcs that aren't allowed in a transition network and sample a dataset from it.

* Run all the algorithms with this dataset and compare the execution times, the percentage of real arcs recovered and the total number of arcs in the resulting networks. For the particle swarm algorithms, 5 runs will be performed and the results will be averaged. The dynamic MMHC algorithm is deterministic, and so no reruns are needed. 

* Keep the same parameters in all runs, the only thing that will change is the original structures and the datasets used. This will allow us to see the effects of increasing both $m$ and $k$ progressively

To be able to replicate this experiments, the following chunk of code has to be executed in order to get the experimental version of the package up and working:

```{r install_github, eval = FALSE, echo = TRUE}
library(devtools)
devtools::install_github("dkesada/natPSOHO")
```

The functions with the suffix '_exp' will be deleted in the final version of the package, and so some of the utils functions used in the experiments will be missing.

### Network and dataset generation

To generate the random network structures, we will sample a random 'natPosition' where each natural number is sampled from a uniform distribution in the interval $[0, 2^m-1]$. This position will be translated to a BN and a dataset will be sampled from it.


```{r init}
library(natPsoho)
library(dbnR)
library(bnlearn)
library(data.table)
```

```{r generation}
n_vars <- 3
size <- 2
seed <- 42
min_mu <- -10
max_mu <- 10
min_sd <- 0.1
max_sd <- 5
min_coef <- -3
max_coef <- 3

# Generates a DBN structure and the sampled dataset
og <- generate_random_network_exp(n_vars, size, min_mu, max_mu, min_sd,
                                   max_sd, min_coef, max_coef, seed)

bnlearn::score(og$net, og$f_dt, type = "bge")

```

The metric that both 'psoho' algorithms use to evaluate structures is the bayesian Gaussian equivalent score (bge) implemented in 'bnlearn'. A network representing the real structure that generated the data obtains a bge score of -238k, while randomly generated networks get bge values around the -27k mark.

```{r natPsoho}

eval_sol <- function(real, sol){
  real_arcs <- apply(real$arcs, 1, function(x){paste0(x[1], x[2])})
  sol_arcs <- apply(sol$arcs, 1, function(x){paste0(x[1], x[2])})

  print(paste0("Number of real arcs: ", length(real_arcs)))
  print(paste0("Number of real arcs in solution: ", sum(real_arcs %in% sol_arcs)))
  print(paste0("Total number of arcs in solution: ", length(sol_arcs)))
}

a <- Sys.time()
#res <- natPsoho::learn_dbn_structure_pso(og$f_dt, size, n_inds = 300, n_it = 50,
#                                      in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
#                                      v_probs = c(10, 65, 25), p = 0.2, r_probs = c(-0.5, 1.5))
res <- dbnR::learn_dbn_struc(NULL, size = size, f_dt = og$f_dt, intra = F)
class(res) <- "bn"
eval_sol(og$net, res)
print(Sys.time() - a)
print(bnlearn::score(res, og$f_dt, type = "bge"))
```

Getting the learned network to have only exactly the real arcs is very difficult because a network with only the 13 correct arcs has a fitness of -237911.3 and one with those 13 arcs and 7 additional ones has a fitness of -237951. Overall, those additional false positives are not too aggravating and don't create a dense network, given that this network can have a maximum of 50 arcs. 

With the strong constraints imposed to intra and inter-slice arcs, a network resulting from this learning algorithm can have $(s-1) * n^2$ maximum arcs, with $s$ being the size of the network and $n$ being the number of nodes. This means that the number of possible arcs scales cuadratically with the number of nodes and linearly with the size of the network as a result of only allowing arcs to t_0.

Next, we will compare the results of both algorithms with the same sampled data and the same parameters:

```{r time_trial}

a <- Sys.time()
res <- natPsoho::learn_dbn_structure_pso(dt, size, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), p = 0.06, r_probs = c(-0.5, 1.5))
eval_sol(struct, res)
print("Elapsed time for the natPsoho algorithm:")
print(Sys.time() - a)
print("Final best score obtained:")
print(bnlearn::score(res, dt, type = "bge"))

a <- Sys.time()
res <- dbnR::learn_dbn_struc(dt, size, method = "psoho", f_dt = dt, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), r_probs = c(-0.5, 1.5))
eval_sol(struct, res)
print("Elapsed time for the binary psoho algorithm:")
print(Sys.time() - a)
print("Final best score obtained:")
class(res) <- "bn"
print(bnlearn::score(res, dt, type = "bge"))
```

Both resulting networks scores are very similar, and the execution time of the 'natPsoho' algorithm is substancially lower. The same solutions can be obtained in both algorithms, and the 'natPsoho' one allows for the discovery of higher order networks on less time.

### Comparison in times between psoho and natPsoho

Now, we will test how well the algorithm scales when dealing with higher orders. To test it, we will train several networks for the 'motor' sample dataset in the 'dbnR' package varying the size. There are 11 variables per time-slice and 3000 instances in the dataset, and the tests will be performed with a particle population of 200 individuals and 10 iterations in total.

```{r bigger size}

dt <- dbnR::motor
size <- 8
dt <- dbnR::fold_dt(dt, size)

a <- Sys.time()
res <- natPsoho::learn_dbn_structure_pso(dt, size, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), p = 0.06, r_probs = c(-0.5, 1.5))
print("Elapsed time for the natPsoho algorithm:")
print(Sys.time() - a)

a <- Sys.time()
res <- dbnR::learn_dbn_struc(dt, size, method = "psoho", f_dt = dt, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), r_probs = c(-0.5, 1.5))
print("Elapsed time for the psoho algorithm:")
print(Sys.time() - a)
```

The time difference is quite noticeable for higher orders. Given that the data structure of the 'natPsoho' algorithm remains constant in size, the initialization is done always in the same time as opposed to the original 'psoho' algorithm. Bitwise operations over integer numbers are also faster than operating through lists of positive/negative 1s and 0s, and the only operation whose cost increases with the size is finding the open positions in an integer. If I could make an algorithm that computes this in constant time, like in the case of the bitcount of integers, the algorithm would be unaffected by the maximum size. 
