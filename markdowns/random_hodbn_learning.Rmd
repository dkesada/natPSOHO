---
title: "Random HO-DBN learning"
author: "David Quesada"
date: "10/2/2021"
output: html_document
---

# Recovering HO-DBN structures from sampled data

In this rmarkdown, we will make some experiments to test the capabilities of the _psoho_, _natPsoho_ and dynamic MMHC algorithms in learning HO-DBNs with increasing Markovian order and number of receiving variables. Specifically, we are going to:

* Randomly generate a HO-DBN structure with order $m$ and $k$ receiving variables, crop the arcs that aren't allowed in a transition network and sample a dataset from it.

* Run all the algorithms with this dataset and compare the execution times, the percentage of real arcs recovered and the total number of arcs in the resulting networks. For the particle swarm algorithms, 5 runs will be performed and the results will be averaged. The dynamic MMHC algorithm is deterministic, and so no reruns are needed. 

* Keep the same parameters in all runs, the only thing that will change is the original structures and the datasets used. This will allow us to see the effects of increasing both $m$ and $k$ progressively

To be able to replicate this experiments, the following chunk of code has to be executed in order to get the experimental version of the package up and working:

```{r install_github, eval = FALSE, echo = TRUE}
library(devtools)
devtools::install_github("dkesada/natPSOHO")
```

The functions with the suffix '_exp' will be deleted in the final version of the package, and so some of the utils functions used in the experiments will be missing.

### Network and dataset generation

To generate the random network structures, we will sample a random 'natPosition' where each natural number is sampled from a uniform distribution in the interval $[0, 2^m-1]$. This position will be translated to a BN and a dataset will be sampled from it.


```{r init}
library(natPsoho)
library(dbnR)
library(bnlearn)
library(data.table)
```

```{r generation}
n_vars <- 3
size <- 3
seed <- 42
min_mu <- -10
max_mu <- 10
min_sd <- 0.1
max_sd <- 5
min_coef <- -3
max_coef <- 3

# Generates a DBN structure and the sampled dataset
og <- generate_random_network_exp(n_vars, size, min_mu, max_mu, min_sd,
                                   max_sd, min_coef, max_coef, seed)

bnlearn::score(og$net, og$f_dt, type = "bge")

```

The metric that both 'psoho' algorithms use to evaluate structures is the bayesian Gaussian equivalent score (bge) implemented in *bnlearn*. A size 3 network with 3 variables in t_0 representing the real structure that generated the data obtains a bge score of -238k, while randomly generated networks get bge values around the -27k mark.

```{r natPsoho}

eval_sol <- function(real, sol, port = NULL){
  real_arcs <- apply(real$arcs, 1, function(x){paste0(x[1], x[2])})
  sol_arcs <- apply(sol$arcs, 1, function(x){paste0(x[1], x[2])})
  
  if(is.null(port)){
    cat(paste0("Number of real arcs: ", length(real_arcs)))
    cat(paste0("Number of real arcs in solution: ", sum(real_arcs %in% sol_arcs)))
    cat(paste0("Total number of arcs in solution: ", length(sol_arcs)))
  }
  
  else{
    writeLines(paste0("Number of real arcs: ", length(real_arcs)), port)
    writeLines(paste0("Number of real arcs in solution: ", sum(real_arcs %in% sol_arcs)), port)
    writeLines(paste0("Total number of arcs in solution: ", length(sol_arcs)), port)
  }
}

a <- Sys.time()
res <- natPsoho::learn_dbn_structure_pso(og$f_dt, size, n_inds = 300, n_it = 50,
                                     in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
                                     v_probs = c(10, 65, 25), p = 0.2, r_probs = c(-0.5, 1.5))
# res <- dbnR::learn_dbn_struc(NULL, size = size, f_dt = og$f_dt, intra = F)
# class(res) <- "bn"
eval_sol(og$net, res)
print(Sys.time() - a)
print(bnlearn::score(res, og$f_dt, type = "bge"))
```
When the number of variables and the size is low, we can expect to find the real structure that generated the data, but as those parameters increase the search space gets immensely huge, and the global optimum is hard to achieve with low particle count and few iterations.

The previous chunk shows the execution of the natPSOHO algorithm in one network. We will now automate the process of generating a network for some size and number of variables, executing all 3 algorithms and printing the results into a log file.

Next, we will compare the results of all algorithms with the same sampled data and the same parameters. The next chunk performs all the experiments and generates a file "results_log.txt" with the metrics. Beware that it takes a really long time to perform all the experiments.

```{r time_trial}

evaluate_network <- function(og, size, n_vars, port){
  writeLines("------------------------------------------------------------ \n", port)
  writeLines(paste0("Session with size ", size, " and ", n_vars, " variables in t_0: \n \n"), port)
  
  # natPSOHO
  writeLines(paste0("Results for the natPSOHO algorithm: \n"), port)
  a <- Sys.time()
  res <- natPsoho::learn_dbn_structure_pso(og$f_dt, size, n_inds = 300, n_it = 50,
                                       in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
                                       v_probs = c(10, 65, 25), p = 0.2, r_probs = c(-0.5, 1.5))
  writeLines(paste0("Elapsed time: ", format(Sys.time() - a)), port)
  eval_sol(og$net, res, port)
  class(res) <- "bn"
  writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bge"))), port)
  writeLines("\n", port)
  
  # PSOHO
  writeLines(paste0("Results for the PSOHO algorithm: \n"), port)
  a <- Sys.time()
  res <- dbnR::learn_dbn_struc(NULL, size, method = "psoho", f_dt = og$f_dt, n_inds = 200, n_it = 50,
                                       in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
                                       v_probs = c(10, 65, 25),  r_probs = c(-0.5, 1.5))
  writeLines(paste0("Elapsed time: ", format(Sys.time() - a)), port)
  eval_sol(og$net, res, port)
  class(res) <- "bn"
  writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bge"))), port)
  writeLines("\n", port)
  
   # DMMHC
  writeLines(paste0("Results for the DMMHC algorithm: \n"), port)
  a <- Sys.time()
  res <- dbnR::learn_dbn_struc(NULL, size, method = "dmmhc", f_dt = og$f_dt, intra = FALSE)
  writeLines(paste0("Elapsed time: ", format(Sys.time() - a)), port)
  eval_sol(og$net, res, port)
  class(res) <- "bn"
  writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bge"))), port)
  writeLines("\n", port)
  
}

max_size <- 8
max_n_vars <- 20
set.seed(42)
port <- file("./results_log.txt", open = "w")

for(size in 2:max_size){
  for(n_vars in 2:max_n_vars){
    og <- generate_random_network_exp(n_vars, size, min_mu, max_mu, min_sd,
                                      max_sd, min_coef, max_coef)
    evaluate_network(og, size, n_vars, port)
  }
}

close(port)

```

Both resulting networks scores are very similar, and the execution time of the 'natPsoho' algorithm is substancially lower. The same solutions can be obtained in both algorithms, and the 'natPsoho' one allows for the discovery of higher order networks on less time.


