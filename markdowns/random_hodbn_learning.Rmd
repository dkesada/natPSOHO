---
title: "Random HO-DBN learning"
author: "David Quesada"
date: "10/2/2021"
output: html_document
---

# Recovering HO-DBN structures from sampled data

In this rmarkdown, we will make some experiments to test the capabilities of the _psoho_, _natPsoho_ and dynamic MMHC algorithms in learning HO-DBNs with increasing Markovian order and number of receiving variables. Specifically, we are going to:

* Randomly generate a HO-DBN structure with order $m$ and $k$ receiving variables, crop the arcs that aren't allowed in a transition network and sample a dataset from it.

* Run all the algorithms with this dataset and compare the execution times, the percentage of real arcs recovered and the total number of arcs in the resulting networks. For the particle swarm algorithms, 5 runs will be performed and the results will be averaged. The dynamic MMHC algorithm is deterministic, and so no reruns are needed. 

* Keep the same parameters in all runs, the only thing that will change is the original structures and the datasets used. This will allow us to see the effects of increasing both $m$ and $k$ progressively

To be able to replicate this experiments, the following chunk of code has to be executed in order to get the experimental version of the package up and working:

```{r install_github, eval = FALSE, echo = TRUE}
library(devtools)
devtools::install_github("dkesada/natPSOHO")
devtools::install_github("dkesada/bnlearn_exp")
```

*WARNING!* The experiments use a highly unstable bnlearn version I tailored specifically for the pso algorithms. Basically, it bypasses all the security checks when scoring a network, adding arcs to a graph and generating an empty graph. It speeds up the execution time greatly, but its use is discouraged outside of this package. Using it will VERY likely result in exceptions and unexpected behaviour. *If you run these experiments, be sure to install a fresh copy of bnlearn afterwards*. I have not decided yet what will I do with these experimental increase in performance one I merge the algorithm into the main package. Its current condition is far too unstable, but the performance improvement is substantial.

The functions with the suffix '_exp' will be deleted in the final version of the package, and so some of the utils functions used in the experiments will be missing.

### Network and dataset generation

To generate the random network structures, we will sample a random 'natPosition' where each natural number is sampled from a uniform distribution in the interval $[0, 2^m-1]$. This position will be translated to a BN and a dataset will be sampled from it.


```{r init}
library(natPsoho)
library(dbnR)
library(bnlearn)
library(data.table)
library(plotly)
```

```{r generation}
n_vars <- 10
size <- 4
seed <- 42
min_mu <- -10
max_mu <- 10
min_sd <- 0.1
max_sd <- 5
min_coef <- -3
max_coef <- 3

# Generates a DBN structure and the sampled dataset
og <- generate_random_network_exp(n_vars, size, min_mu, max_mu, min_sd,
                                   max_sd, min_coef, max_coef, seed)

bnlearn::score(og$net, og$f_dt, type = "bge")

```
The metric that both 'psoho' algorithms use to evaluate structures is the bayesian Gaussian equivalent score (bge) implemented in *bnlearn*. A size 3 network with 3 variables in t_0 representing the real structure that generated the data obtains a bge score of -238k, while randomly generated networks get bge values around the -27k mark.

### Evaluation of a solution

The solutions will be evaluated in terms of the number of real arcs recovered and the execution time. To see these results, we will write the metrics obtained by each solution in a log file.

```{r natPsoho}

eval_sol <- function(real, sol, port = NULL){
  real_arcs <- apply(real$arcs, 1, function(x){paste0(x[1], x[2])})
  sol_arcs <- apply(sol$arcs, 1, function(x){paste0(x[1], x[2])})
  
  if(is.null(port)){
    cat(paste0("Number of real arcs: ", length(real_arcs), "\n"))
    cat(paste0("Number of real arcs in solution: ", sum(real_arcs %in% sol_arcs), "\n"))
    cat(paste0("Total number of arcs in solution: ", length(sol_arcs), "\n"))
  }
  
  else{
    writeLines(paste0("Number of real arcs: ", length(real_arcs)), port)
    writeLines(paste0("Number of real arcs in solution: ", sum(real_arcs %in% sol_arcs)), port)
    writeLines(paste0("Total number of arcs in solution: ", length(sol_arcs)), port)
  }
}

a <- Sys.time()
res <- natPsoho::learn_dbn_structure_pso(og$f_dt, size, n_inds = 300, n_it = 50,
                                     in_cte = 0.8, gb_cte = 0.1, lb_cte = 0.8,
                                     v_probs = c(10, 65, 25), p = 0.2, r_probs = c(-0.5, 1.5), cte = T)
# res <- dbnR::learn_dbn_struc(NULL, size = size, f_dt = og$f_dt, intra = F)
# class(res) <- "bn"
eval_sol(og$net, res)
print(Sys.time() - a)
print(bnlearn::score(res, og$f_dt, type = "bge"))
```
When the number of variables and the size is low, we can expect to find the real structure that generated the data, but as those parameters increase the search space gets immensely huge, and the global optimum is hard to achieve with low particle count and few iterations.

The previous chunk shows the execution of the natPSOHO algorithm in one network. We will now automate the process of generating a network for some size and number of variables, executing all 3 algorithms and printing the results into a log file.

Next, we will compare the results of all algorithms with the same sampled data and the same parameters. The next chunk performs all the experiments and generates a file "results_log.txt" with the metrics. Beware that it takes a really long time to perform all the experiments.

### Experimental comparison

```{r time_trial}

evaluate_network <- function(og, size, n_vars, port){
  writeLines("------------------------------------------------------------ \n", port)
  writeLines(paste0("Session with size ", size, " and ", n_vars, " variables in t_0: \n \n"), port)
  
  # natPSOHO
  writeLines(paste0("Results for the natPSOHO algorithm: \n"), port)
  a <- Sys.time()
  res <- natPsoho::learn_dbn_structure_pso(og$f_dt, size, n_inds = 300, n_it = 50,
                                       in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
                                       #in_cte = 0.9, gb_cte = 0.1, lb_cte = 0.6,
                                       v_probs = c(10, 65, 25), p = 2e-6, r_probs = c(-0.5, 1.5), cte = TRUE)
  a <- format(Sys.time() - a)
  writeLines(paste0("Elapsed time: ", a), port)
  eval_sol(og$net, res, port)
  class(res) <- "bn"
  writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bge"))), port)
  writeLines("\n", port)

  # PSOHO
  writeLines(paste0("Results for the PSOHO algorithm: \n"), port)
  b <- Sys.time()
  res <- dbnR::learn_dbn_struc(NULL, size, method = "psoho", f_dt = og$f_dt, n_inds = 300, n_it = 50,
                                       in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
                                       v_probs = c(10, 65, 25),  r_probs = c(-0.5, 1.5), score = "bge")
  b <- format(Sys.time() - b)
  writeLines(paste0("Elapsed time: ", b), port)
  eval_sol(og$net, res, port)
  class(res) <- "bn"
  writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bge"))), port)
  writeLines("\n", port)
  
  # DMMHC
  # Size 4 with 20 variables takes 21 hours. It is going to scale to weeks by the end of the tests
  # writeLines(paste0("Results for the DMMHC algorithm: \n"), port)
  # a <- Sys.time()
  # res <- dbnR::learn_dbn_struc(NULL, size, method = "dmmhc", f_dt = og$f_dt, intra = FALSE)
  # writeLines(paste0("Elapsed time: ", format(Sys.time() - a)), port)
  # eval_sol(og$net, res, port)
  # class(res) <- "bn"
  # writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bic-g"))), port)
  # writeLines("\n", port)
}

max_size <- 3
#max_size <- 8
vars_set <- c(5, 10, 15, 20)
#vars_set <- c(10,20)
psoho_t <- c()
natPSOHO_t <- c()
psoho_arcs <- c()
natPSOHO_arcs <- c()
port <- file("./results_log_low.txt", open = "w")

for(size in 2:max_size){
  for(n_vars in vars_set){
    set.seed(size + n_vars * 1000) # All runs are seeded in this manner to allow independent reruns
    og <- generate_random_network_exp(n_vars, size, min_mu, max_mu, min_sd,
                                      max_sd, min_coef, max_coef)
    evaluate_network(og, size, n_vars, port)
  }
}

close(port)

```
```{r time_trial_extreme_order}

evaluate_network <- function(og, size, n_vars, port){
  writeLines("------------------------------------------------------------ \n", port)
  writeLines(paste0("Session with size ", size, " and ", n_vars, " variables in t_0: \n \n"), port)
  
  # natPSOHO
  writeLines(paste0("Results for the natPSOHO algorithm: \n"), port)
  a <- Sys.time()
  res <- natPsoho::learn_dbn_structure_pso(og$f_dt, size, n_inds = 300, n_it = 50,
                                       in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
                                       #in_cte = 0.9, gb_cte = 0.1, lb_cte = 0.6,
                                       v_probs = c(10, 65, 25), p = 0, r_probs = c(-0.5, 1.5), cte = TRUE)
  a <- format(Sys.time() - a)
  writeLines(paste0("Elapsed time: ", a), port)
  eval_sol(og$net, res, port)
  class(res) <- "bn"
  writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bge"))), port)
  writeLines("\n", port)

  # PSOHO
  writeLines(paste0("Results for the PSOHO algorithm: \n"), port)
  b <- Sys.time()
  res <- dbnR::learn_dbn_struc(NULL, size, method = "psoho", f_dt = og$f_dt, n_inds = 300, n_it = 50,
                                       in_cte = 0.7, gb_cte = 0.5, lb_cte = 0.5,
                                       v_probs = c(10, 65, 25),  r_probs = c(-0.5, 1.5), score = "bge")
  b <- format(Sys.time() - b)
  writeLines(paste0("Elapsed time: ", b), port)
  eval_sol(og$net, res, port)
  class(res) <- "bn"
  writeLines(paste0("Final best score obtained:", format(bnlearn::score(res, og$f_dt, type = "bge"))), port)
  writeLines("\n", port)
}

max_size_set <- c(10, 15, 20, 25, 30)
#max_size_set <- c(10)
n_vars <- 10
port <- file("./results_log_high_fix.txt", open = "w")

for(size in max_size_set){
  #set.seed(size + n_vars * 1000) # All runs are seeded in this manner to allow independent reruns
  og <- generate_random_network_exp(n_vars, size, min_mu, max_mu, min_sd,
                                    max_sd, min_coef, max_coef)
  evaluate_network(og, size, n_vars, port)
}

close(port)

```

The execution time of the 'natPsoho' algorithm is substantially lower in high-order networks, and similar in smaller ones. The DMMHC algorithm performs quite well for smaller networks, but it scales very poorly, taking up to days to learn dense networks. It's interesting to see that no false arcs are recovered with the DMMHC algorithm. The recovered networks of the PSOHO and natPSOHO algorithms are of equivalent characteristics, which is to be expected given that the score used is the same.


```{r graphs}

# networks <- c("3/5", "3/10", "3/20", "4/5", "4/10", "4/20", "5/5", "5/10", "5/20", "6/5", "6/10", "6/20", "7/5", "7/10", "7/20")
networks <- c("1", "2", "3", "4", "5", "6")
psoho_t <- c(4.22, 23.15, 37.06, 62.58, 77.75, 122.64)
natPSOHO_t <- c(5.81, 14.24, 23.8, 38.02, 57.45, 83.4)
psoho_arcs <- c((118 * 100) / 201, (215 * 100) / 398, (308 * 100) / 616, (423 * 100) / 825, (425 * 100) / 982, (517 * 100) / 1171)
natPSOHO_arcs <- c((152 * 100) / 201, (288 * 100) / 398, (398 * 100) / 616, (533 * 100) / 825, (622 * 100) / 982, (739 * 100) / 1171) 
xform <- list(title = "Markovian order",
              categoryorder = "array",
              categoryarray = networks, titlefont = list(size = 36))
tmp <- data.frame(networks, natPSOHO_t, psoho_t)

fig <- plot_ly(tmp, x = ~networks, y = ~natPSOHO_t, type = 'bar', name = 'natPSOHO',
               marker = list(color = 'rgb(48,102,177)'))
fig <- fig %>% add_trace(y = ~psoho_t, name = 'PSOHO', marker = list(color = 'rgb(230,126,53)'))
fig <- fig %>% layout(xaxis = xform, yaxis = list(title = 'Execution time (m)', titlefont = list(size = 36)), 
                      barmode = 'group', font = list(size = 36))

fig
```


### Profiling



```{r profiling}

tmp_foo <- function(){
  port <- file("./results_log_high_fix.txt", open = "w")

  for(size in max_size_set){
    set.seed(size + n_vars * 1000) # All runs are seeded in this manner to allow independent reruns
    og <- generate_random_network_exp(n_vars, size, min_mu, max_mu, min_sd,
                                      max_sd, min_coef, max_coef)
    evaluate_network(og, size, n_vars, port)
  }
  
  close(port)
}

n_vars <- 10
max_size_set <- 5

Rprof(tmp <- tempfile())
tmp_foo()
Rprof()
summaryRprof(tmp)

```

