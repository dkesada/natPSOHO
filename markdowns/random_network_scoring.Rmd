---
title: "Random network scoring"
output: html_document
---

# Random generation of a network, sampling of a dataset and network score testing 

In this rmarkdown, we will make some experiments to test the capabilities of both the 'psoho' and 'natPsoho' in learning DBNs. Specifically, we are going to:

* Create a network structure for a size 3 DBN and randomly generate a dataset for it. Afterwards, it will allow us to learn structures with it knowing beforehand the real structure of that network.

* See how well can both 'psoho' algorithms recreate this network structure from the dataset consisting on 10k instances.

* Compare their execution times

### Network and dataset generation

To generate the network and the structure, first we define the original structure of the network. We could also generate this network randomly, but this way it is easier to control the tests and evaluate the solutions obtained. Moreover, the 'psoho' algorithms can't learn all network structure because of the constraints they impose on inter-slice and intra-slice arcs. 

We will define the structure in the 'bnlearn' syntax and then generate some data for each variable as if they were normally distributed. This will create a dataset of samples from variables with a fixed mean and variance. Afterwards, we can represent the arcs in the network by fixing some coefficients and multiplying the columns in the dataset following the structure of the network, i.e, multiplying the children nodes by a coefficient and the parent nodes.

```{r init}
library(natPsoho)
library(bnlearn)
library(data.table)
```

```{r generation}
ordering <- c("A", "B", "C", "D", "E")
size <- 3

set.seed(42)

struct <- bnlearn::model2network("[A_t_2][B_t_2][C_t_2][D_t_2][E_t_2][A_t_1][B_t_1][C_t_1][D_t_1][E_t_1][A_t_0|C_t_2:B_t_1][B_t_0|C_t_2:E_t_2:A_t_1][C_t_0|B_t_1:C_t_1][D_t_0|A_t_2:D_t_1][E_t_0|D_t_2:A_t_1:C_t_1:D_t_1]")

dt = data.table(A_t_2 = rnorm(10000, 2.9, 2.7),
               B_t_2 = rnorm(10000, 1.2, 0.3),
               C_t_2 = rnorm(10000, 1.2, 1.9),
               D_t_2 = rnorm(10000, 3.7, 1.5),
               E_t_2 = rnorm(10000, 1.7, 0.6),
               A_t_1 = rnorm(10000, 2.8, 2.6),
               B_t_1 = rnorm(10000, 1.3, 0.5),
               C_t_1 = rnorm(10000, 1.5, 2),
               D_t_1 = rnorm(10000, 3.6, 1.6),
               E_t_1 = rnorm(10000, 1.7, 0.7),
               A_t_0 = rnorm(10000, 2.8, 2.6),
               B_t_0 = rnorm(10000, 1.3, 0.5),
               C_t_0 = rnorm(10000, 1.5, 2),
               D_t_0 = rnorm(10000, 3.6, 1.6),
               E_t_0 = rnorm(10000, 1.7, 0.7))

dt[, A_t_0 := A_t_0 + (-0.29) * C_t_2 + 1.3 * B_t_1]
dt[, B_t_0 := B_t_0 + 0.14 * C_t_2 + 0.87 * E_t_2 + (-0.7) * A_t_1]
dt[, C_t_0 := C_t_0 + 1.3 * C_t_1 + -(0.3) * B_t_1]
dt[, D_t_0 := D_t_0 + 0.37 * A_t_2 + 1.1 * D_t_1]
dt[, E_t_0 := E_t_0 + 0.21 * D_t_2 + 0.63 * A_t_1 + (-0.7) * C_t_1 + 0.2 * D_t_1]

print(bnlearn::score(struct, dt, type = "bge"))

```

The metric that both 'psoho' algorithms use to evaluate structures is the bayesian Gaussian equivalent score (bge) implemented in 'bnlearn'. A network representing the real structure that generated the data obtains a bge score of -237k, while randomly generated networks get bge values around the -27k mark.

```{r natPsoho}

eval_sol <- function(real, sol){
  real_arcs <- apply(real$arcs, 1, function(x){paste0(x[1], x[2])})
  sol_arcs <- apply(sol$arcs, 1, function(x){paste0(x[1], x[2])})

  print(paste0("Number of real arcs: ", length(real_arcs)))
  print(paste0("Number of real arcs in solution: ", sum(real_arcs %in% sol_arcs)))
  print(paste0("Total number of arcs in solution: ", length(sol_arcs)))
}

a <- Sys.time()
res <- natPsoho::learn_dbn_structure_pso(dt, size, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), p = 0.06, r_probs = c(-0.5, 1.5))
eval_sol(struct, res)
print(Sys.time() - a)
print(bnlearn::score(res, dt, type = "bge"))
```

Getting the learned network to have only exactly the real arcs is very difficult because a network with only the 13 correct arcs has a fitness of -237911.3 and one with those 13 arcs and 7 additional ones has a fitness of -237951. Overall, those additional false positives are not too aggravating and don't create a dense network, given that this network can have a maximum of 50 arcs. 

With the strong constraints imposed to intra and inter-slice arcs, a network resulting from this learning algorithm can have $(s-1) * n^2$ maximum arcs, with $s$ being the size of the network and $n$ being the number of nodes. This means that the number of possible arcs scales cuadratically with the number of nodes and linearly with the size of the network as a result of only allowing arcs to t_0.

Next, we will compare the results of both algorithms with the same sampled data and the same parameters:

```{r time_trial}

a <- Sys.time()
res <- natPsoho::learn_dbn_structure_pso(dt, size, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), p = 0.06, r_probs = c(-0.5, 1.5))
eval_sol(struct, res)
print("Elapsed time for the natPsoho algorithm:")
print(Sys.time() - a)
print("Final best score obtained:")
print(bnlearn::score(res, dt, type = "bge"))

a <- Sys.time()
res <- dbnR::learn_dbn_struc(dt, size, method = "psoho", f_dt = dt, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), r_probs = c(-0.5, 1.5))
eval_sol(struct, res)
print("Elapsed time for the binary psoho algorithm:")
print(Sys.time() - a)
print("Final best score obtained:")
class(res) <- "bn"
print(bnlearn::score(res, dt, type = "bge"))
```

Both resulting networks scores are very similar, and the execution time of the 'natPsoho' algorithm is substancially lower. The same solutions can be obtained in both algorithms, and the 'natPsoho' one allows for the discovery of higher order networks on less time.

### Comparison in times between psoho and natPsoho

Now, we will test how well the algorithm scales when dealing with higher orders. To test it, we will train several networks for the 'motor' sample dataset in the 'dbnR' package varying the size. There are 11 variables per time-slice and 3000 instances in the dataset, and the tests will be performed with a particle population of 200 individuals and 10 iterations in total.

```{r bigger size}

dt <- dbnR::motor
size <- 8
dt <- dbnR::fold_dt(dt, size)

a <- Sys.time()
res <- natPsoho::learn_dbn_structure_pso(dt, size, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), p = 0.06, r_probs = c(-0.5, 1.5))
print("Elapsed time for the natPsoho algorithm:")
print(Sys.time() - a)

a <- Sys.time()
res <- dbnR::learn_dbn_struc(dt, size, method = "psoho", f_dt = dt, n_inds = 200, n_it = 10,
                                      in_cte = 0.8, gb_cte = 0.5, lb_cte = 0.5,
                                      v_probs = c(10, 65, 25), r_probs = c(-0.5, 1.5))
print("Elapsed time for the psoho algorithm:")
print(Sys.time() - a)
```

The time difference is quite noticeable for higher orders. Given that the data structure of the 'natPsoho' algorithm remains constant in size, the initialization is done always in the same time as opposed to the original 'psoho' algorithm. Bitwise operations over integer numbers are also faster than operating through lists of positive/negative 1s and 0s, and the only operation whose cost increases with the size is finding the open positions in an integer. If I could make an algorithm that computes this in constant time, like in the case of the bitcount of integers, the algorithm would be unaffected by the maximum size. 
